#!/usr/bin/octave -qfif (nargin!=3)  printf("Usage: ./experiment.m <data> <alphas> <bes>\n");  exit(1);endarg_list=argv();data=arg_list{1};as=str2num(arg_list{2}); % sacamos las alphasbs=str2num(arg_list{3}); % sacamos la betasload(data); [N,L]=size(data); %filas, columnas D=L-1;ll=unique(data(:,L)); %vector clases unicasC=numel(ll); %numero de clases distintasrand("seed",23);data=data(randperm(N),:); %barajamos la filasNTr=round(.7*N); % cogemos el 70 por cien para trainM=N-NTr; %numeros de datos - numero datos de train --> tamaño muestra testte=data(NTr+1:N,:);% el 30 por cien restantes es para testprintf("#      b   E   k Ete       Intervalo \n");printf("#------- --- --- ---   -------------\n");for a=as  for b=bs    [w,E,k]=perceptron(data(1:NTr,:),b,a); %entrenamos el perceptron con los datos de train    rl=zeros(M,1);    for n=1:M       rl(n)=ll(linmach(w,[1 te(n,1:D)]'));      end    [nerr m]=confus(te(:,L),rl); %nerr es el numero de errores    p = nerr / M; % probabilidad de error M--> tamaño muestra test    s =sqrt(p*(1-p)/M);    r = 1.96*s; % intervalo al 95%     printf("%8.1f %8.1f %3d %3d %3d %7.1f [%.1f, %.1f]\n",a,b,E,k,nerr,p*100,(p-r)*100,(p+r)*100);      endend    